{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b2f936",
   "metadata": {},
   "source": [
    "# Midterm 1\n",
    "\n",
    "## FINM 36700 - 2023\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28e7fd",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd63ff",
   "metadata": {},
   "source": [
    "## Please note the following:\n",
    "\n",
    "Points\n",
    "* The exam is 100 points.\n",
    "* You have 120 minutes to complete the exam.\n",
    "* For every minute late you submit the exam, you will lose one point.\n",
    "Final Exam\n",
    "\n",
    "Submission\n",
    "* You will upload your solution to the `Midterm 1` assignment on Canvas, where you downloaded this. (Be sure to **submit** on Canvas, not just **save** on Canvas.\n",
    "* Your submission should be readable, (the graders can understand your answers,) and it should **include all code used in your analysis in a file format that the code can be executed.** \n",
    "\n",
    "Rules\n",
    "* The exam is open-material, closed-communication.\n",
    "* You do not need to cite material from the course github repo--you are welcome to use the code posted there without citation.\n",
    "\n",
    "Advice\n",
    "* If you find any question to be unclear, state your interpretation and proceed. We will only answer questions of interpretation if there is a typo, error, etc.\n",
    "* The exam will be graded for partial credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960a3c5",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**All data files are found in the class github repo, in the `data` folder.**\n",
    "\n",
    "This exam makes use of the following data files:\n",
    "* `midterm_data_1.xlsx`\n",
    "\n",
    "This file has sheets for...\n",
    "* `info` - names of each stock ticker\n",
    "* `excess returns` - weekly excess returns on several stocks\n",
    "* `SPY` - weekly excess returns on SPY\n",
    "\n",
    "Note the data is **weekly** so any annualizations should use `52` weeks in a year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f6568",
   "metadata": {},
   "source": [
    "#### If useful\n",
    "here is code to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important Functions\n",
    "#Covering all imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5cb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_summary(return_data, annualization = 12):\n",
    "    \"\"\" \n",
    "        Returns the Performance Stats for given set of returns\n",
    "        Inputs: \n",
    "            return_data - DataFrame with Date index and Monthly Returns for different assets/strategies.\n",
    "        Output:\n",
    "            summary_stats - DataFrame with annualized mean return, vol, sharpe ratio. Skewness, Excess Kurtosis, Var (0.5) and\n",
    "                            CVaR (0.5) and drawdown based on monthly returns. \n",
    "    \"\"\"\n",
    "    summary_stats = return_data.mean().to_frame('Mean').apply(lambda x: x*annualization)\n",
    "    summary_stats['Volatility'] = return_data.std().apply(lambda x: x*np.sqrt(annualization))\n",
    "    summary_stats['Sharpe Ratio'] = summary_stats['Mean']/summary_stats['Volatility']\n",
    "\n",
    "    summary_stats['Skewness'] = return_data.skew()\n",
    "    summary_stats['Excess Kurtosis'] = return_data.kurtosis()\n",
    "    summary_stats['VaR (0.05)'] = return_data.quantile(.05, axis = 0)\n",
    "    summary_stats['CVaR (0.05)'] = return_data[return_data <= return_data.quantile(.05, axis = 0)].mean()\n",
    "    \n",
    "    wealth_index = 1000*(1+return_data).cumprod()\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks)/previous_peaks\n",
    "\n",
    "    summary_stats['Max Drawdown'] = drawdowns.min()\n",
    "    summary_stats['Peak'] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns]\n",
    "    summary_stats['Bottom'] = drawdowns.idxmin()\n",
    "    \n",
    "    recovery_date = []\n",
    "    for col in wealth_index.columns:\n",
    "        prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T\n",
    "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
    "    summary_stats['Recovery'] = recovery_date\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def mvo_performance_stats(asset_returns,cov_matrix,port_weights, port_type,period):\n",
    "    \"\"\" \n",
    "        Returns the Annualized Performance Stats for given asset returns, portfolio weights and covariance matrix\n",
    "        Inputs: \n",
    "            asset_return - Excess return over the risk free rate for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "            port_weights = weights of the assets in the portfolio (1 x n) Vector\n",
    "            port_type = Type of Portfolio | Eg - Tangency or Mean-Variance Portfolio\n",
    "            period = Monthly frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = np.dot(port_weights,asset_returns)\n",
    "    vol = np.sqrt(port_weights @ cov_matrix @ port_weights.T)*np.sqrt(period)\n",
    "    sharpe = ret/vol\n",
    "\n",
    "    stats = pd.DataFrame([[ret,vol,sharpe]],columns= [\"Annualized Return\",\"Annualized Volatility\",\"Annualized Sharpe Ratio\"], index = [port_type])\n",
    "    return stats\n",
    "\n",
    "def tangency_portfolio_rfr(asset_return,cov_matrix, cov_diagnolize = False):\n",
    "    \"\"\" \n",
    "        Returns the tangency portfolio weights in a (1 x n) vector\n",
    "        Inputs: \n",
    "            asset_return - return for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "    \"\"\"\n",
    "    if cov_diagnolize:\n",
    "        asset_cov = np.diag(np.diag(cov_matrix))\n",
    "    else:\n",
    "        asset_cov = np.array(cov_matrix)\n",
    "    inverted_cov= np.linalg.inv(asset_cov)\n",
    "    one_vector = np.ones(len(cov_matrix.index))\n",
    "    \n",
    "    den = (one_vector @ inverted_cov) @ (asset_return)\n",
    "    num =  inverted_cov @ asset_return\n",
    "    return (1/den) * num\n",
    "\n",
    "def mv_portfolio_rfr(asset_return,cov_matrix,target_ret,tangency_port):\n",
    "    \"\"\" \n",
    "        Returns the Mean-Variance portfolio weights in a (1 x n) vector when a riskless assset is available\n",
    "        Inputs: \n",
    "            asset_return - Excess return over the risk free rate for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "            target_ret = Target Return (Annualized)\n",
    "            tangency_port = Tangency portfolio when a riskless assset is available\n",
    "    \"\"\"\n",
    "    asset_cov = np.array(cov_matrix)\n",
    "    inverted_cov= np.linalg.inv(asset_cov)\n",
    "    one_vector = np.ones(len(cov_matrix.index))\n",
    "    \n",
    "    delta_den = (asset_return.T @ inverted_cov) @ (asset_return)\n",
    "    delta_num = (one_vector @ inverted_cov) @ (asset_return)\n",
    "    delta_tilde = (delta_num/delta_den) * target_ret\n",
    "    return (delta_tilde * tangency_port)\n",
    "\n",
    "def gmv_portfolio(asset_return,cov_matrix):\n",
    "    \"\"\" \n",
    "        Returns the Global Minimum Variance portfolio weights in a (1 x n) vector\n",
    "        Inputs: \n",
    "            asset_return - return for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "    \"\"\"\n",
    "    asset_cov = np.array(cov_matrix)\n",
    "    inverted_cov= np.linalg.inv(asset_cov)\n",
    "    one_vector = np.ones(len(cov_matrix.index))\n",
    "    \n",
    "    den = (one_vector @ inverted_cov) @ (one_vector)\n",
    "    num =  inverted_cov @ one_vector\n",
    "    return (1/den) * num\n",
    "\n",
    "def mv_portfolio(asset_return,cov_matrix,target_ret,tangency_port):\n",
    "    \"\"\" \n",
    "        Returns the Mean-Variance portfolio weights in a (1 x n) vector when no riskless assset is available\n",
    "        Inputs: \n",
    "            asset_return - total return for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "            target_ret = Target Return (Not-Annualized)\n",
    "            tangency_port = Tangency portfolio\n",
    "    \"\"\"\n",
    "    omega_tan = tangency_portfolio_rfr(asset_return.mean(),cov_matrix)\n",
    "    omega_gmv = gmv_portfolio(asset_return,cov_matrix) \n",
    "    \n",
    "    mu_tan = asset_return.mean() @ omega_tan\n",
    "    mu_gmv = asset_return.mean() @ omega_gmv\n",
    "    \n",
    "    delta = (target_ret - mu_gmv)/(mu_tan - mu_gmv)\n",
    "    mv_weights = delta * omega_tan + (1-delta)*omega_gmv\n",
    "    return mv_weights\n",
    "\n",
    "def regression_based_performance(factor,fund_ret,rf,constant = True):\n",
    "    \"\"\" \n",
    "        Returns the Regression based performance Stats for given set of returns and factors\n",
    "        Inputs:\n",
    "            factor - Dataframe containing monthly returns of the regressors\n",
    "            fund_ret - Dataframe containing monthly excess returns of the regressand fund\n",
    "            rf - Monthly risk free rate of return\n",
    "        Output:\n",
    "            summary_stats - (Beta of regression, treynor ratio, information ratio, alpha). \n",
    "    \"\"\"\n",
    "    if constant:\n",
    "        X = sm.tools.add_constant(factor)\n",
    "    else:\n",
    "        X = factor\n",
    "    y=fund_ret\n",
    "    model = sm.OLS(y,X,missing='drop').fit()\n",
    "    \n",
    "    if constant:\n",
    "        beta = model.params[1:]\n",
    "        alpha = round(float(model.params['const']),6)\n",
    "        \n",
    "    else:\n",
    "        beta = model.params\n",
    "    treynor_ratio = ((fund_ret.values-rf.values).mean()*12)/beta[0]\n",
    "    tracking_error = (model.resid.std()*np.sqrt(12))\n",
    "    if constant:        \n",
    "        information_ratio = model.params[0]*12/tracking_error\n",
    "    r_squared = model.rsquared\n",
    "    if constant:\n",
    "        return (beta,treynor_ratio,information_ratio,alpha,r_squared,tracking_error)\n",
    "    else:\n",
    "        return (beta,treynor_ratio,r_squared,tracking_error)\n",
    "    \n",
    "\n",
    "def rolling_regression_param(factor,fund_ret,roll_window = 60):\n",
    "    \"\"\" \n",
    "        Returns the Rolling Regression parameters for given set of returns and factors\n",
    "        Inputs:\n",
    "            factor - Dataframe containing monthly returns of the regressors\n",
    "            fund_ret - Dataframe containing monthly excess returns of the regressand fund\n",
    "            roll_window = rolling window for regression\n",
    "        Output:\n",
    "            params - Dataframe with time-t as the index and constant and Betas as columns\n",
    "    \"\"\"\n",
    "    X = sm.add_constant(factor)\n",
    "    y= fund_ret\n",
    "    rols = RollingOLS(y, X, window=roll_window)\n",
    "    rres = rols.fit()\n",
    "    params = rres.params.copy()\n",
    "    params.index = np.arange(1, params.shape[0] + 1)\n",
    "    return params\n",
    "    \n",
    "def calc_probability_lowret(num_years,mean_ret_check,mean_ret,vol):\n",
    "        \"\"\" \n",
    "        Returns the Probability that the cumulative market return will fall short of the cumulative\n",
    "        risk-free return for each period\n",
    "        Inputs: \n",
    "            mean - annualized mean returns of market for a period.\n",
    "            vol - annualized volatility of returns for a period\n",
    "            num_years - Number of years to calculate\n",
    "        Output:\n",
    "            probability - DataFrame with probability for each period (step = 1)\n",
    "        \"\"\"\n",
    "        lst = []\n",
    "        for n in range (0,num_years+1,1):\n",
    "            norm_val = np.sqrt(n)*(mean_ret_check - mean_ret)/(vol)\n",
    "            prob = (norm.cdf(norm_val))*100\n",
    "            lst.append(pd.DataFrame([[n,prob]],columns=['Time','Probability(%)']))\n",
    "        probability = pd.concat(lst)\n",
    "        return probability\n",
    "\n",
    "def calc_return_metrics(data, as_df=False, adj=12):\n",
    "    \"\"\"\n",
    "    Calculate return metrics for a DataFrame of assets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame of asset returns.\n",
    "        as_df (bool, optional): Return a DF or a dict. Defaults to False (return a dict).\n",
    "        adj (int, optional): Annualization. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        Union[dict, DataFrame]: Dict or DataFrame of return metrics.\n",
    "    \"\"\"\n",
    "    summary = dict()\n",
    "    summary[\"Annualized Return\"] = data.mean() * adj\n",
    "    summary[\"Annualized Volatility\"] = data.std() * np.sqrt(adj)\n",
    "    summary[\"Annualized Sharpe Ratio\"] = (\n",
    "        summary[\"Annualized Return\"] / summary[\"Annualized Volatility\"]\n",
    "    )\n",
    "    summary[\"Annualized Sortino Ratio\"] = summary[\"Annualized Return\"] / (\n",
    "        data[data < 0].std() * np.sqrt(adj)\n",
    "    )\n",
    "    return pd.DataFrame(summary, index=data.columns) if as_df else summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344abdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEIN = '../data/midterm_1_data.xlsx'\n",
    "sheet_exrets = 'excess returns'\n",
    "sheet_spy = 'spy'\n",
    "\n",
    "retsx = pd.read_excel(FILEIN, sheet_name=sheet_exrets).set_index('date')\n",
    "spy = pd.read_excel(FILEIN, sheet_name=sheet_spy).set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af33e1",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "| Problem | Points |\n",
    "|---------|--------|\n",
    "| 1       | 20     |\n",
    "| 2       | 35     |\n",
    "| 3       | 30     |\n",
    "| 4       | 15     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158c236",
   "metadata": {},
   "source": [
    "### Each numbered question is worth 5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362090a7",
   "metadata": {},
   "source": [
    "### Notation\n",
    "(Hidden LaTeX commands)\n",
    "\n",
    "$$\\newcommand{\\mux}{\\tilde{\\boldsymbol{\\mu}}}$$\n",
    "$$\\newcommand{\\wtan}{\\boldsymbol{\\text{w}}^{\\text{tan}}}$$\n",
    "$$\\newcommand{\\wtarg}{\\boldsymbol{\\text{w}}^{\\text{port}}}$$\n",
    "$$\\newcommand{\\mutarg}{\\tilde{\\boldsymbol{\\mu}}^{\\text{port}}}$$\n",
    "$$\\newcommand{\\wEW}{\\boldsymbol{\\text{w}}^{\\text{EW}}}$$\n",
    "$$\\newcommand{\\wRP}{\\boldsymbol{\\text{w}}^{\\text{RP}}}$$\n",
    "$$\\newcommand{\\wREG}{\\boldsymbol{\\text{w}}^{\\text{REG}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40926a81",
   "metadata": {},
   "source": [
    "# 1. Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0d60",
   "metadata": {},
   "source": [
    "### No Data Needed\n",
    "\n",
    "These problem does not require any data file. Rather, analyze the situation conceptually, based on the information below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086f971",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "In what sense was ProShares `HDG` successful in hedging the `HFRI`, and in what sense was it unsuccessful in tracking the `HFRI`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b0e61",
   "metadata": {},
   "source": [
    "HDG tracks a modified version of the ML Factor Model, MLFM-ES. The Merrill Lynch Factor Model involves indexes which cannot be exactly traded. For that reason, ProShares created a traded version of the Factor Model which replaces non-traded indexes with liquid, traded securities. But "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ddf91",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "We discussed multiple ways of calculating Value-at-Risk (VaR). What are the tradeoffs between using the normal distribution formula versus a directly empirical approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ef8ef",
   "metadata": {},
   "source": [
    "\n",
    "**Normal Distribution:**\n",
    "\n",
    "1. The normal distribution method assumes that asset returns follow a Gaussian (normal) distribution. This implies that returns are symmetric, unimodal, and have constant volatility, which are unrealistic. \n",
    "2. The normal distribution method is a parametric approach that relies on estimating the mean and standard deviation of returns. While it simplifies the calculation, it can be sensitive to outliers or data deviations.\n",
    "3. The normal distribution method is relatively simple and computationally efficient.\n",
    "\n",
    "**Empirical Approach:**\n",
    "\n",
    "1. Takes no assumptions and uses historical data, which could be better especicially when normal distributions style of data is unrealistic. \n",
    "2. The empirical approach is non-parametric, which means it does not rely on estimated parameters such as mean and standard deviation. It can capture non-normal and fat-tailed return distributions.\n",
    "3. The empirical approach can be more flexible in capturing the actual characteristics of the return distribution, including skewness and kurtosis, making it suitable for a wide range of assets.\n",
    "\n",
    "**Tradeoffs:**\n",
    "Which method we use depends on what we want out of our measures. Some trade-offs include:\n",
    "1. **Assumption vs. Data:** The normal distribution approach simplifies the modeling process but relies on the assumption of normality. The empirical approach avoids such assumptions but requires a sufficiently long and relevant historical dataset.\n",
    "2. **Accuracy vs. Robustness:** The normal distribution method can be more accurate if returns follow a normal distribution. However, it may perform poorly during extreme market events. The empirical approach is more robust but may be less accurate if the historical data does not capture future scenario or if we do not have enough data to work with. \n",
    "3. **Computational Complexity:** The empirical approach can be computationally more intensive, especially when dealing with large datasets, as it involves sorting and analyzing historical observations. The normal distribution approach is simpler in terms of computation.\n",
    "4. **Backtesting and Validation:** Both methods require rigorous backtesting and validation to assess their accuracy and reliability. The empirical approach may require more extensive backtesting due to its non-parametric nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b7e68",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Did we find that **TIPS** have been useful in expanding the mean-variance frontier in the past? Did we conclude they might be useful in the future? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771b6a7",
   "metadata": {},
   "source": [
    "In our original homework 1, we concluded that:\n",
    "* Dropping TIPS from the investment set barely impacts the weights or the resulting performance.\n",
    "* Adjusting the mean of TIPS upward even just 1 standard error substantially impacts the allocations and moderately boosts the resulting performance.\n",
    "\n",
    "Based on just a mean-variance analysis, it seems one could reasonably go either way with TIPS as an alternate asset class. In the argument to keep it separate, there is more diversification between TIPS and bonds than between SPY and many other equity buckets Harvard has. On the other hand, TIPS mostly impact the allocation to domestic bonds and might be seen as another asset in that bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97348b",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "What aspect of the classic mean-variance optimization approach leads to extreme answers? How did regularization help with this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f3925",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4bc91c",
   "metadata": {},
   "source": [
    "The classic mean-variance optimization approach, while powerful, can lead to extreme or impractical portfolio allocations, especially in cases where there is a high degree of noise or multicollinearity in the data. This issue arises because mean-variance optimization aims to find the portfolio with the highest Sharpe ratio, which can result in significant concentration in a few assets or even full allocation to a single asset.\n",
    "\n",
    "Regularization, such as L2 regularization (ridge regularization), helps with this issue in the following ways:\n",
    "\n",
    "1. **Reduces Extreme Allocations**: Regularization introduces a penalty term that discourages extreme or concentrated portfolio allocations. This penalty term is based on the sum of the squared values of the portfolio weights. By minimizing the weighted sum of squares, regularization helps prevent the optimization process from allocating an impractical amount to a single asset.\n",
    "\n",
    "2. **Mitigates Overfitting**: Regularization mitigates overfitting by constraining the model's complexity. In the context of portfolio optimization, this means that the optimization process is less likely to overemphasize the historical returns of a specific asset. Regularization prevents the model from fitting the historical data too closely, leading to allocations that are more robust and balanced.\n",
    "\n",
    "3. **Improved Stability**: Regularization adds stability to the optimization process. Without regularization, small changes in the input data can lead to significant changes in the optimal portfolio weights. Regularization dampens these fluctuations and provides a smoother and more stable allocation.\n",
    "\n",
    "4. **Accounting for Data Imperfections**: Regularization acknowledges the presence of noise or errors in the data. In practice, financial data can be noisy, and regularization helps the optimizer account for this noise without allocating disproportionately to assets with unusually high past returns.\n",
    "\n",
    "In summary, regularization introduces a degree of \"smoothing\" into the optimization process, resulting in more balanced and practical portfolio allocations. It helps prevent the extreme outcomes that can be associated with classic mean-variance optimization when applied to real-world financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e42346",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66389e",
   "metadata": {},
   "source": [
    "# 2. Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b67603",
   "metadata": {},
   "source": [
    "Consider a mean-variance optimization of **excess** returns provided in `midterm_1_data.xlsx.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe496df",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Report the following **annualized** statistics:\n",
    "* mean\n",
    "* volatility\n",
    "* Sharpe ratio\n",
    "\n",
    "Which assets have the highest / lowest Sharpe ratios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51bb55d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.055270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.097769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>0.106984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.010956</td>\n",
       "      <td>0.084179</td>\n",
       "      <td>0.130154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.039368</td>\n",
       "      <td>0.156035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.033311</td>\n",
       "      <td>0.166318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.064913</td>\n",
       "      <td>0.192760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mean  Volatility  Sharpe Ratio\n",
       "XOM    0.002388    0.043213      0.055270\n",
       "GOOGL  0.003718    0.038027      0.097769\n",
       "AMZN   0.004605    0.043043      0.106984\n",
       "TSLA   0.010956    0.084179      0.130154\n",
       "AAPL   0.006143    0.039368      0.156035\n",
       "MSFT   0.005540    0.033311      0.166318\n",
       "NVDA   0.012513    0.064913      0.192760"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = performance_summary(retsx, annualization = 1)\n",
    "sorted_summary = summary[['Mean', 'Volatility', 'Sharpe Ratio']].sort_values(by='Sharpe Ratio', ascending=True)\n",
    "sorted_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f370ded",
   "metadata": {},
   "source": [
    "XOM has the lowest Sharpe while NVDA has the highest Sharpe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f091c84",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Report the weights of the tangency portfolio.\n",
    "\n",
    "Also report the Sharpe ratio achieved by the tangency portfolio over this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ed7ad6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tangency Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.322605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.787496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.228607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0.495996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-0.502721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.105975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.019257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tangency Weight\n",
       "AAPL          0.322605\n",
       "MSFT          0.787496\n",
       "AMZN         -0.228607\n",
       "NVDA          0.495996\n",
       "GOOGL        -0.502721\n",
       "TSLA          0.105975\n",
       "XOM           0.019257"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtan = pd.DataFrame(tangency_portfolio_rfr(summary['Mean'], retsx.cov()), index = summary.index, columns=['Tangency Weight'])\n",
    "wtan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bab65",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "* What weight is given to the asset with the lowest Sharpe ratio?\n",
    "* What Sharpe ratio does the lowest (most negative) weight asset have?\n",
    "\n",
    "Explain. Support your answer with evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f53738",
   "metadata": {},
   "source": [
    "XOM has the lowest sharpe but has the a 0.019257 weightage while the GOOGL has a negative weightage of -0.502721, which is the lowest of all the assets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131963d5",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "Let's examine the out-of-sample performance.\n",
    "\n",
    "Calculate and report the following three allocations using only data through the end of 2022:\n",
    "* tangency portfolio\n",
    "* equally weighted portfolio\n",
    "* a regularized approach, with a new formula shown below\n",
    "\n",
    "where\n",
    "$$\\wEW_i = \\frac{1}{n}$$\n",
    "\n",
    "$$\\wREG \\sim \\widehat{\\Sigma}^{-1}\\mux$$\n",
    "\n",
    "$$\\widehat{\\Sigma} = \\frac{\\Sigma + \\boldsymbol{2}\\,\\Sigma_D}{\\boldsymbol{3}}$$\n",
    "where $\\Sigma_D$ denotes a *diagonal* matrix of the security variances, with zeros in the off-diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cebf21c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangency Portfolio Weights:\n",
      "        Tangency Weight\n",
      "AAPL          0.322605\n",
      "MSFT          0.787496\n",
      "AMZN         -0.228607\n",
      "NVDA          0.495996\n",
      "GOOGL        -0.502721\n",
      "TSLA          0.105975\n",
      "XOM           0.019257\n",
      "Equally Weighted Portfolio Weights:\n",
      "          Weight\n",
      "AAPL   0.142857\n",
      "MSFT   0.142857\n",
      "AMZN   0.142857\n",
      "NVDA   0.142857\n",
      "GOOGL  0.142857\n",
      "TSLA   0.142857\n",
      "XOM    0.142857\n",
      "Regularized Portfolio Weights:\n",
      "          Weight\n",
      "AAPL   2.175852\n",
      "MSFT   2.715273\n",
      "AMZN   0.418795\n",
      "NVDA   1.646301\n",
      "GOOGL  0.141063\n",
      "TSLA   0.734261\n",
      "XOM    0.779944\n"
     ]
    }
   ],
   "source": [
    "end_date = '2022-12-31'\n",
    "data_end_date = '2022-12-31'  # The end of data for calculation\n",
    "\n",
    "# Filter the data for the desired date range\n",
    "data = retsx[:end_date]\n",
    "\n",
    "# Calculate expected returns (use mean of returns as an example)\n",
    "expected_returns = data.mean()\n",
    "\n",
    "# Calculate covariance matrix (use cov function as an example)\n",
    "cov_matrix = retsx.cov()\n",
    "\n",
    "# Step 3: Calculate portfolio weights\n",
    "\n",
    "# Equally Weighted Portfolio\n",
    "n = len(expected_returns)\n",
    "equally_weighted_weights = np.array([1 / n] * n)\n",
    "\n",
    "# Regularized Portfolio\n",
    "# Calculate the regularized covariance matrix as per your formula\n",
    "Sigma_D = np.diag(np.diag(cov_matrix))\n",
    "regularized_cov_matrix = (cov_matrix + 2 * Sigma_D) / 3\n",
    "inv_regularized_cov_matrix = np.linalg.inv(regularized_cov_matrix)\n",
    "regularized_weights = inv_regularized_cov_matrix @ expected_returns\n",
    "\n",
    "# Print or return the calculated portfolio weights\n",
    "equally_weighted_weights_df = pd.DataFrame(equally_weighted_weights, columns=['Weight'], index=expected_returns.index)\n",
    "regularized_weights_df = pd.DataFrame(regularized_weights, columns=['Weight'], index=expected_returns.index)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Tangency Portfolio Weights:\\n\", wtan)\n",
    "print(\"Equally Weighted Portfolio Weights:\\n\", equally_weighted_weights_df)\n",
    "print(\"Regularized Portfolio Weights:\\n\", regularized_weights_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9255bdd",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "Report the out-of-sample (2023) performance of all three portfolios in terms of annualized mean, vol, and Sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd9305fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Performance in 2023:\n",
      "Tangency Portfolio - Mean Return: 1.4920\n",
      "Equally Weighted Portfolio - Mean Return: 0.9551\n",
      "Regularized Portfolio - Mean Return: 8.5855\n",
      "Tangency Portfolio - Volatility: 0.4074\n",
      "Equally Weighted Portfolio - Volatility: 0.2425\n",
      "Regularized Portfolio - Volatility: 2.0747\n",
      "Tangency Portfolio - Sharpe Ratio: 3.6620\n",
      "Equally Weighted Portfolio - Sharpe Ratio: 3.9386\n",
      "Regularized Portfolio - Sharpe Ratio: 4.1383\n"
     ]
    }
   ],
   "source": [
    "returns_2023 = retsx['2022-12-31':]\n",
    "\n",
    "# Calculate portfolio returns for 2023\n",
    "tangency_portfolio_returns_2023 = np.dot(returns_2023, wtan)\n",
    "equally_weighted_portfolio_returns_2023 = np.dot(returns_2023, equally_weighted_weights)\n",
    "regularized_portfolio_returns_2023 = np.dot(returns_2023, regularized_weights)\n",
    "\n",
    "# Calculate portfolio statistics for 2023\n",
    "mean_2023 = {\n",
    "    \"Tangency Portfolio\": tangency_portfolio_returns_2023.mean() * 52,\n",
    "    \"Equally Weighted Portfolio\": equally_weighted_portfolio_returns_2023.mean() * 52,\n",
    "    \"Regularized Portfolio\": regularized_portfolio_returns_2023.mean() * 52,\n",
    "}\n",
    "\n",
    "volatility_2023 = {\n",
    "    \"Tangency Portfolio\": tangency_portfolio_returns_2023.std() * np.sqrt(52),\n",
    "    \"Equally Weighted Portfolio\": equally_weighted_portfolio_returns_2023.std() * np.sqrt(52),\n",
    "    \"Regularized Portfolio\": regularized_portfolio_returns_2023.std() * np.sqrt(52),\n",
    "}\n",
    "\n",
    "sharpe_ratio_2023 = {\n",
    "    \"Tangency Portfolio\": mean_2023[\"Tangency Portfolio\"] / volatility_2023[\"Tangency Portfolio\"],\n",
    "    \"Equally Weighted Portfolio\": mean_2023[\"Equally Weighted Portfolio\"] / volatility_2023[\"Equally Weighted Portfolio\"],\n",
    "    \"Regularized Portfolio\": mean_2023[\"Regularized Portfolio\"] / volatility_2023[\"Regularized Portfolio\"],\n",
    "}\n",
    "\n",
    "# Print the out-of-sample portfolio performance for 2023\n",
    "print(\"Portfolio Performance in 2023:\")\n",
    "for portfolio, mean in mean_2023.items():\n",
    "    print(f\"{portfolio} - Mean Return: {mean:.4f}\")\n",
    "for portfolio, vol in volatility_2023.items():\n",
    "    print(f\"{portfolio} - Volatility: {vol:.4f}\")\n",
    "for portfolio, sharpe in sharpe_ratio_2023.items():\n",
    "    print(f\"{portfolio} - Sharpe Ratio: {sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65554c5",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "Imagine just for this problem that this data is for **total** returns, not excess returns.\n",
    "\n",
    "Report the weights of the global-minimum-variance portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "defd2d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMV Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.206231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.491250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.160866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>-0.119168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.011378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>-0.046927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>0.296369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GMV Weight\n",
       "AAPL     0.206231\n",
       "MSFT     0.491250\n",
       "AMZN     0.160866\n",
       "NVDA    -0.119168\n",
       "GOOGL    0.011378\n",
       "TSLA    -0.046927\n",
       "XOM      0.296369"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmv = pd.DataFrame(gmv_portfolio(retsx,retsx.cov()), index = summary.index, columns=['GMV Weight'])\n",
    "gmv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41a996",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "To target a mean return of 0.005%, would you be long or short this global minimum variance portfolio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2631129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00347408])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmv_ret = np.dot(summary['Mean'], gmv)\n",
    "gmv_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee282e",
   "metadata": {},
   "source": [
    "Since the GMV's return is more than the mean target return, we would be long the GMV but we would onyl want to be long a certain percentage of our portfolio in GMV, with the rest in risk free or uninvested, to achieve our target returns with less volatility assumed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdbfa9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d9fbd",
   "metadata": {},
   "source": [
    "# 3. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426af123",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Report the following performance metrics of excess returns for Tesla (`TSLA`).\n",
    "* skewness\n",
    "* kurtosis\n",
    "\n",
    "You are not annualizing any of these stats.\n",
    "\n",
    "What do these metrics indicate about the nature of the returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6a032f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = performance_summary(retsx, annualization = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f664d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mean                          0.010956\n",
       "Volatility                    0.084179\n",
       "Sharpe Ratio                  0.130154\n",
       "Skewness                      0.441455\n",
       "Excess Kurtosis               1.527376\n",
       "VaR (0.05)                   -0.122519\n",
       "CVaR (0.05)                  -0.155313\n",
       "Max Drawdown                 -0.682185\n",
       "Peak               2021-11-05 00:00:00\n",
       "Bottom             2023-01-06 00:00:00\n",
       "Recovery                           NaT\n",
       "Name: TSLA, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.loc['TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a911b",
   "metadata": {},
   "source": [
    "The skewness tells us that TSLA generally has more returns that are positively skewed, suggesting tails of high returns. The kurtosis indicates that the tails of TSLA's returns are fairly fat, indicating that more outlier events occur or events that are very postive or negative happen more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877253fc",
   "metadata": {},
   "source": [
    "## 2. \n",
    "\n",
    "Report the maximum drawdown for `TSLA` over the sample.\n",
    "* Ignore that your data is in excess returns rather than total returns.\n",
    "* Simply proceed with the excess return data for this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f5ea51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6821852296331565"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.loc['TSLA', 'Max Drawdown']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d77a74",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "For `TSLA`, calculate the following metrics, relative to `SPY`:\n",
    "* market beta\n",
    "* alpha\n",
    "* sortino ratio\n",
    "\n",
    "Annualize alpha and sortino ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ff6b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1.7768245314443845\n",
      "alpha: 0.3094703790703374\n",
      "sortino: 1.4091507431551036\n"
     ]
    }
   ],
   "source": [
    "excess_tsla = pd.DataFrame(retsx['TSLA'] - spy['SPY'])\n",
    "tsla_ret_metrics = calc_return_metrics(excess_tsla, as_df = True, adj = 52)\n",
    "\n",
    "X = spy['SPY']\n",
    "Y = retsx['TSLA']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "beta = model.params['SPY']\n",
    "alpha = model.params['const'] * 52\n",
    "sortino = tsla_ret_metrics['Annualized Sortino Ratio'][0]\n",
    "\n",
    "print(f'beta: {beta}\\nalpha: {alpha}\\nsortino: {sortino}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7919b2",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "Continuing with `TSLA`, calculate the full-sample, 5th-percentile CVaR.\n",
    "* Use the `normal` formula, assuming mean returns are zero.\n",
    "* Use the full-sample volatility.\n",
    "\n",
    "Use the entire sample to calculate a single CVaR number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89a6b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA CVaR(0.05): -0.15531312622504762\n"
     ]
    }
   ],
   "source": [
    "#this question did not ask for annualized number, so I am leaving it out...\n",
    "print('TSLA CVaR(0.05):', summary['CVaR (0.05)'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94065e1d",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "Now calculate the 5th-percentile, one-period ahead, **VaR** for `TSLA`.\n",
    "\n",
    "Here, calculate the running series of VaR estimates.\n",
    "\n",
    "Again, \n",
    "* use the normal formula, with mean zero.\n",
    "\n",
    "But now, use the rolling volatility, based on \n",
    "* rolling window or $m=52$ weeks.\n",
    "\n",
    "Report the final 5 values of your calculated VaR series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8679f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 5 values of rolling VaR series:\n",
      "date\n",
      "2023-06-16   -0.156149\n",
      "2023-06-23   -0.156265\n",
      "2023-06-30   -0.153732\n",
      "2023-07-07   -0.152232\n",
      "2023-07-14   -0.150509\n",
      "Name: TSLA, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rolling_volatility = np.sqrt((retsx['TSLA']**2).rolling(52).mean().shift())\n",
    "\n",
    "rolling_VaR = -1.645 * rolling_volatility  # 5th percentile\n",
    "\n",
    "final_5_VaR_values = rolling_VaR.tail(5)\n",
    "\n",
    "print(\"Final 5 values of rolling VaR series:\")\n",
    "print(final_5_VaR_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a30cac",
   "metadata": {},
   "source": [
    "## 6. \n",
    "\n",
    "Calculate the out-of-sample **hit ratio** for your VaR series reported in your previous answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1a2e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample Hit Ratio: 0.04846938775510204\n"
     ]
    }
   ],
   "source": [
    "rolling_VaR = -1.645 * rolling_volatility\n",
    "\n",
    "hits = (retsx['TSLA'] < rolling_VaR).sum()\n",
    "total_observations = len(retsx['TSLA'])\n",
    "\n",
    "hit_ratio = hits / total_observations\n",
    "\n",
    "print(\"Out-of-sample Hit Ratio:\", hit_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee2bb2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3820d1b",
   "metadata": {},
   "source": [
    "# 4. Hedging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559f9a9",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Consider the following scenario: you are holding a \\$100 million long position in `NVDA`. You wish to hedge the position using some combination of \n",
    "* `AAPL`\n",
    "* `AMZN`\n",
    "* `GOOGL`\n",
    "* `MSFT`\n",
    "\n",
    "Report the positions you would hold of those 4 securities for an optimal hedge.\n",
    "\n",
    "Note:\n",
    "* In the regression estimation, include an intercept.\n",
    "* Use the full-sample regression. No need to worry about in-sample versus out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b5ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Hedge Positions:\n",
      "AAPL     3.416865e+07\n",
      "AMZN     4.172599e+07\n",
      "GOOGL   -7.847952e+05\n",
      "MSFT     5.878967e+07\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "nvda_returns = retsx['NVDA']\n",
    "other_securities_returns = retsx[['AAPL', 'AMZN', 'GOOGL', 'MSFT']]\n",
    "\n",
    "# Add a constant (intercept) to the independent variables\n",
    "X = sm.add_constant(other_securities_returns)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = sm.OLS(nvda_returns, X).fit()\n",
    "\n",
    "# Get the coefficients of the regression\n",
    "hedge_ratios = model.params[1:]  # Exclude the intercept\n",
    "\n",
    "# Calculate the optimal hedge positions\n",
    "total_investment = 100e6  # $100 million\n",
    "hedge_positions = hedge_ratios * total_investment\n",
    "\n",
    "# Display the optimal hedge positions\n",
    "print(\"Optimal Hedge Positions:\")\n",
    "print(hedge_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127f366",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "How well does the hedge do? Cite a regression statistic to support your answer.\n",
    "\n",
    "Also estimate the volatility of the basis, (epsilon.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a575b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R): 0.4581682361155385\n",
      "Volatility of the Basis (epsilon): 0.04778208768803615\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R-squared value\n",
    "r_squared = model.rsquared\n",
    "print(\"R-squared (R):\", r_squared)\n",
    "\n",
    "# Calculate the residuals (basis)\n",
    "residuals = model.resid\n",
    "\n",
    "# Calculate the standard deviation (volatility) of the residuals\n",
    "epsilon_volatility = residuals.std()\n",
    "print(\"Volatility of the Basis (epsilon):\", epsilon_volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b6394",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "Report the annualized intercept. By including this intercept, what are you assuming about the nature of the returns of `NVDA` as well as the returns of the hedging instruments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b6737de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Intercept: 0.2737521756167878\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the intercept in weekly terms\n",
    "weekly_intercept = model.params['const']\n",
    "\n",
    "# Annualize the intercept using the factor for weekly data\n",
    "annualized_intercept = weekly_intercept * 52\n",
    "print(\"Annualized Intercept:\", annualized_intercept)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
